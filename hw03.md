I chose to use beautiful soup because he Wikipedia tables I needed are rendered fully on the server side, which means I didn’t have to fight with JavaScript or build a more heavyweight Scrapy project. That meant I could grab the HTML, point Beautiful Soup at the <table class="wikitable"> elements, and have working data in minutes.I built small, focused scripts—one for each booster family that pull out each row, normalize dates, and write into CSV files. Grouping the three Falcon Heavy cores by flight number was just a matter of collecting rows into buckets and writing out only those with exactly three entries. My prompts to ScrapeGraphAI were to show the exact column headers, give one example line, then to extract only these columns into CSV. I did most of my editing in VS Code, sometimes using Jupyter notebooks for quick checks, and used Python’s requests for fetching pages and Beautiful Soup for parsing. I started the project with the simplest Block 1/1.1 case, then add Block 4 and Block 5, then assemble the Falcon Heavy reports, and finally write special summaries for “fastest” and “longest” turnarounds. My hardest thign I had to do was the Ai stuff that was really confusing. 
